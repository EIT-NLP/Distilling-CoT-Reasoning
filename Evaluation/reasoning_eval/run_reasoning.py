import json
import torch
import re
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer
from prompt_utils import get_prompt
import utils

# Define available dtypes
DTYPES = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}

def load_qa_data(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data

def extract_answer_label(generated_text):
    # 提取答案标签，可以匹配模式："The answer is D:"、"the answer is D:"、"The answer is True/False"
    answer_pattern = re.search(r'[Tt]he answer is ([A-E]|True|False)', generated_text)
    if answer_pattern:
        return answer_pattern.group(1)
    return None

def clean_generated_output(generated_text):
    # Clean the generated output by removing any repetitive phrases or unwanted text patterns.
    cleaned_text = re.sub(r'<start_of_turn>.*?<end_of_turn>', '', generated_text, flags=re.DOTALL)
    cleaned_text = re.sub(r'\n+', '\n', cleaned_text).strip()  # Remove extra newlines and whitespace
    cleaned_text = re.sub(r'(?:\bedReader\b|\bmodel\b)+', '', cleaned_text).strip()  # Remove redundant tokens
    # Remove user and model indicators and other unwanted patterns from the output
    stop_phrases = ["USER:", "ASSISTANT:", "### Instruction:", "Response:", "<start_of_turn>", "[INST]", "<|eot_id|>", "####"]
    for stop in stop_phrases:
        cleaned_text = cleaned_text.replace(stop, "").strip()
    # print("DEBUG: Generated answer after initial clean-up:", cleaned_text)
    return cleaned_text

def generate_answer_vllm(llm, sampling_params, prompt):
    # Generate responses using vllm
    output = llm.generate([prompt], sampling_params)
    # print("DEBUG: Raw output structure from llm.generate:", output)
    if output and output[0].outputs:
        response_text = output[0].outputs[0].text
        # print("DEBUG: Generated response text:", response_text)
        return response_text
    else:
        # print("DEBUG: No text generated by the model.")
        return ""

def main(model_path, data_path, output_path, max_length, form, dtype):
    # Load the model and tokenizer
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # load tokenizer
    model_name_lower = args.model.lower()
    tokenizer_path = args.model
    print("DEBUG:", model_name_lower)
    if 'bloom' in model_name_lower:
        from transformers import BloomTokenizerFast
        tokenizer = BloomTokenizerFast.from_pretrained(tokenizer_path,use_fast=True)
    else:
        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
    
    # Set up LLM and sampling parameters
    stop_tokens = ["USER:", "ASSISTANT:", "### Instruction:", "Response:", "<start_of_turn>", "[INST]", "\n\nProblem", "\nProblem", "Problem:", "<|eot_id|>", "####"]
    sampling_params = SamplingParams(temperature=0, use_beam_search=True, best_of=5, max_tokens=max_length, stop=stop_tokens)
    
    llm = LLM(model=model_path, tensor_parallel_size=torch.cuda.device_count(), dtype=dtype, trust_remote_code=True)
    # print("DEBUG: Model loaded successfully.")
    # Load the dataset
    dataset = load_qa_data(data_path)

    # Open the output file for writing results incrementally
    with open(output_path, "w", encoding="utf-8") as file_handle:
        correct_predictions = 0
        token_lengths = []  # List to store token lengths for each output

        # Iterate over the dataset and generate predictions
        for entry in dataset:
            instruction = entry["instruction"]
            true_label = entry.get("label", "")

            # Wrap the instruction in a list of tuples to match the expected format for get_prompt
            qas = [(instruction, "")]

            # Generate the prompt based on the selected prompt style
            prompt, _ = get_prompt(qas, form)
            # print("DEBUG: Prompt", prompt)
            # Generate the answer using vllm
            generated_answer = generate_answer_vllm(llm, sampling_params, prompt)
            # print("DEBUG: Generated answer before cleaning:", generated_answer)

            # Clean the generated answer to remove unwanted text
            cleaned_generated_answer = clean_generated_output(generated_answer)
            

            # Calculate token length of the cleaned generated answer
            tokenized_output = tokenizer(cleaned_generated_answer, return_tensors='pt', truncation=True)
            token_length = len(tokenized_output['input_ids'][0])
            token_lengths.append(token_length)

            # Extract the label from the cleaned answer
            generated_label = extract_answer_label(cleaned_generated_answer)
            # generated_label = cleaned_generated_answer
            # print("DEBUG: Generated answer after initial clean-up:", cleaned_generated_answer)

            
            # Check if the generated label matches the true label
            is_correct = generated_label == true_label
            if is_correct:
                correct_predictions += 1

            # Store the result immediately after generating the answer
            result = {
                "question": instruction,
                "correct": true_label,
                "solution": cleaned_generated_answer,
                "pred": generated_label,
                "is_correct": is_correct,
                "token_length": token_length
            }
            file_handle.write(json.dumps(result) + '\n')
            file_handle.flush()  # Ensure the data is written to disk immediately

        # Print accuracy and token length statistics
        accuracy = correct_predictions / len(dataset) if len(dataset) > 0 else 0
        print(f'Final accuracy: {accuracy:.4f}')
        if token_lengths:
            max_length = max(token_lengths)
            min_length = min(token_lengths)
            avg_length = sum(token_lengths) / len(token_lengths)
            print(f"Output token lengths - Max: {max_length}, Min: {min_length}, Avg: {avg_length:.2f}")

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Run QA dataset evaluation with vLLM.")
    parser.add_argument("--model", required=True, type=str, help="Path of the pretrained model.")
    parser.add_argument("--dataset", required=True, type=str, help="Path to the dataset (e.g., strategyQA_test.json, openbookQA_test.json, csqa_test.json).")
    parser.add_argument("--output", type=str, required=True, help="Path to save the output results.")
    parser.add_argument("--dtype", default='bfloat16', type=str, choices=list(DTYPES.keys()), help="Data type for model (float32, bfloat16, float16).")
    parser.add_argument("--model_max_length", default=320, type=int, help="Maximum length for model generation.")
    parser.add_argument("--form", default='alpaca', type=str, help="Style of prompt to use for generation.")

    args = parser.parse_args()
    args.dtype = DTYPES[args.dtype]

    main(
        model_path=args.model,
        data_path=args.dataset,
        output_path=args.output,
        max_length=args.model_max_length,
        form=args.form,
        dtype=args.dtype
    )
